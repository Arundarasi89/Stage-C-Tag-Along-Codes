{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Classification and Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a linear classifier as a binary classifier that seperates 2 classes -> + and - class using a linear separator by computing a linear combination of the features and comparing against a set threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic Regression: Sigmoid, logit, log-likeliehood\n",
    "    \n",
    "Logistic regression is a linear algorithm that can be used for binary or multiclass classification. It is a discriminative classifier that estimates the probability that an instance belongs to a class using an s-shape function curve called the sigmoid function.\n",
    "The predicted values obtained after using a linear equation on the predictors by applying logistic regression can fall in the range of negative infinity to positive infinity. The sigmoid maps these results by shrinking the value to fall between 0 and 1. We can say that we use the sigmoid function to transform linear regression into logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The sigmoid function can be applied to a linear function.\n",
    "z = B0 + B1x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "For a binary classification task with classes A and B, if a threshold is set for 0.5 and the probability of an instance belonging to a class is p, we can say that if p<0.5 the instance if of class A while it is of class B is p>0.5.\n",
    "Also known as the log of odds, logit is the logarithm of odds ratio where the odds ratio is the probability that an event occurs divided by the probability that the event does not occur. \n",
    "Logit is the inverse of the sigmoid such that it maps values from negative infinity to positive infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv( 'https://query.data.world/s/wh6j7rxy2hvrn4ml75ci62apk5hgae' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.199546</td>\n",
       "      <td>0.097188051</td>\n",
       "      <td>0.036888</td>\n",
       "      <td>0.029320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.032351e-01</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>AreaTotHA</td>\n",
       "      <td>483000.000000</td>\n",
       "      <td>687000.000000</td>\n",
       "      <td>334600</td>\n",
       "      <td>127000.000000</td>\n",
       "      <td>100943.000800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.732543e+06</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>BiocapPerCap</td>\n",
       "      <td>0.159804</td>\n",
       "      <td>0.135261</td>\n",
       "      <td>0.084003213</td>\n",
       "      <td>0.013742</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.262086e-01</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>550176.242700</td>\n",
       "      <td>465677.972200</td>\n",
       "      <td>289207.1078</td>\n",
       "      <td>47311.551720</td>\n",
       "      <td>114982.279300</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.467355e+06</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>1992</td>\n",
       "      <td>1</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>0.387510</td>\n",
       "      <td>0.189462</td>\n",
       "      <td>1.26E-06</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.033398</td>\n",
       "      <td>1.114093</td>\n",
       "      <td>1.728629e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   country  year  country_code        record      crop_land   grazing_land  \\\n",
       "0  Armenia  1992             1    AreaPerCap       0.140292       0.199546   \n",
       "1  Armenia  1992             1     AreaTotHA  483000.000000  687000.000000   \n",
       "2  Armenia  1992             1  BiocapPerCap       0.159804       0.135261   \n",
       "3  Armenia  1992             1  BiocapTotGHA  550176.242700  465677.972200   \n",
       "4  Armenia  1992             1  EFConsPerCap       0.387510       0.189462   \n",
       "\n",
       "   forest_land  fishing_ground  built_up_land    carbon         total QScore  \n",
       "0  0.097188051        0.036888       0.029320  0.000000  5.032351e-01     3A  \n",
       "1       334600   127000.000000  100943.000800  0.000000  1.732543e+06     3A  \n",
       "2  0.084003213        0.013742       0.033398  0.000000  4.262086e-01     3A  \n",
       "3  289207.1078    47311.551720  114982.279300  0.000000  1.467355e+06     3A  \n",
       "4     1.26E-06        0.004165       0.033398  1.114093  1.728629e+00     3A  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 72186 entries, 0 to 72185\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   country         72186 non-null  object \n",
      " 1   year            72186 non-null  int64  \n",
      " 2   country_code    72186 non-null  int64  \n",
      " 3   record          72186 non-null  object \n",
      " 4   crop_land       51714 non-null  float64\n",
      " 5   grazing_land    51714 non-null  float64\n",
      " 6   forest_land     51714 non-null  object \n",
      " 7   fishing_ground  51713 non-null  float64\n",
      " 8   built_up_land   51713 non-null  float64\n",
      " 9   carbon          51713 non-null  float64\n",
      " 10  total           72177 non-null  float64\n",
      " 11  QScore          72185 non-null  object \n",
      "dtypes: float64(6), int64(2), object(4)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51481\n",
       "2A    10576\n",
       "2B    10096\n",
       "1B       16\n",
       "1A       16\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['QScore'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country               0\n",
       "year                  0\n",
       "country_code          0\n",
       "record                0\n",
       "crop_land         20472\n",
       "grazing_land      20472\n",
       "forest_land       20472\n",
       "fishing_ground    20473\n",
       "built_up_land     20473\n",
       "carbon            20473\n",
       "total                 9\n",
       "QScore                1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country           0\n",
       "year              0\n",
       "country_code      0\n",
       "record            0\n",
       "crop_land         0\n",
       "grazing_land      0\n",
       "forest_land       0\n",
       "fishing_ground    0\n",
       "built_up_land     0\n",
       "carbon            0\n",
       "total             0\n",
       "QScore            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51473\n",
       "2A      224\n",
       "1A       16\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['QScore'].value_counts()\n",
    "#highly imbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for now we use class 1A and 2A\n",
    "df['QScore'] = df['QScore'].replace(['1A'],'2A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    51473\n",
       "2A      240\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['QScore'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>2.072989e-01</td>\n",
       "      <td>8.112722e-01</td>\n",
       "      <td>0.048357265</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>2.998367e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.119497e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>AreaTotHA</td>\n",
       "      <td>8.417600e+06</td>\n",
       "      <td>3.294260e+07</td>\n",
       "      <td>1963600</td>\n",
       "      <td>917100.000000</td>\n",
       "      <td>1.217520e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.545842e+07</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>BiocapPerCap</td>\n",
       "      <td>2.021916e-01</td>\n",
       "      <td>2.636077e-01</td>\n",
       "      <td>0.027166736</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>2.924496e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.301590e-01</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>8.210214e+06</td>\n",
       "      <td>1.070408e+07</td>\n",
       "      <td>1103135.245</td>\n",
       "      <td>322736.916200</td>\n",
       "      <td>1.187524e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.152769e+07</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>6.280528e-01</td>\n",
       "      <td>1.810332e-01</td>\n",
       "      <td>0.162800822</td>\n",
       "      <td>0.014729</td>\n",
       "      <td>2.924496e-02</td>\n",
       "      <td>1.391455</td>\n",
       "      <td>2.407316e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country  year  country_code        record     crop_land  grazing_land  \\\n",
       "1536  Algeria  2016             4    AreaPerCap  2.072989e-01  8.112722e-01   \n",
       "1537  Algeria  2016             4     AreaTotHA  8.417600e+06  3.294260e+07   \n",
       "1538  Algeria  2016             4  BiocapPerCap  2.021916e-01  2.636077e-01   \n",
       "1539  Algeria  2016             4  BiocapTotGHA  8.210214e+06  1.070408e+07   \n",
       "1540  Algeria  2016             4  EFConsPerCap  6.280528e-01  1.810332e-01   \n",
       "\n",
       "      forest_land  fishing_ground  built_up_land    carbon         total  \\\n",
       "1536  0.048357265        0.022585   2.998367e-02  0.000000  1.119497e+00   \n",
       "1537      1963600   917100.000000   1.217520e+06  0.000000  4.545842e+07   \n",
       "1538  0.027166736        0.007948   2.924496e-02  0.000000  5.301590e-01   \n",
       "1539  1103135.245   322736.916200   1.187524e+06  0.000000  2.152769e+07   \n",
       "1540  0.162800822        0.014729   2.924496e-02  1.391455  2.407316e+00   \n",
       "\n",
       "     QScore  \n",
       "1536     2A  \n",
       "1537     2A  \n",
       "1538     2A  \n",
       "1539     2A  \n",
       "1540     2A  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#order the dataset based on class 2A\n",
    "df_2A = df[df['QScore']=='2A']\n",
    "df_2A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20780</th>\n",
       "      <td>France</td>\n",
       "      <td>1992</td>\n",
       "      <td>68</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>9.485874e-01</td>\n",
       "      <td>4.375167e-01</td>\n",
       "      <td>0.737739791</td>\n",
       "      <td>2.017436e-01</td>\n",
       "      <td>1.559900e-01</td>\n",
       "      <td>3.066358</td>\n",
       "      <td>5.547935e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56478</th>\n",
       "      <td>Somalia</td>\n",
       "      <td>1965</td>\n",
       "      <td>201</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>1.464603e-01</td>\n",
       "      <td>1.792926e+00</td>\n",
       "      <td>0.391813062</td>\n",
       "      <td>7.486020e-04</td>\n",
       "      <td>1.989028e-02</td>\n",
       "      <td>0.035686</td>\n",
       "      <td>2.387525e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56887</th>\n",
       "      <td>Somalia</td>\n",
       "      <td>2016</td>\n",
       "      <td>201</td>\n",
       "      <td>EFConsTotGHA</td>\n",
       "      <td>1.723746e+06</td>\n",
       "      <td>5.411010e+06</td>\n",
       "      <td>5865820.964</td>\n",
       "      <td>6.550573e+04</td>\n",
       "      <td>3.311375e+05</td>\n",
       "      <td>477086.269000</td>\n",
       "      <td>1.387431e+07</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12195</th>\n",
       "      <td>Chile</td>\n",
       "      <td>1995</td>\n",
       "      <td>40</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>5.149702e+06</td>\n",
       "      <td>7.393071e+06</td>\n",
       "      <td>35781483.45</td>\n",
       "      <td>6.758648e+06</td>\n",
       "      <td>1.007553e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.609046e+07</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9015</th>\n",
       "      <td>Burundi</td>\n",
       "      <td>2010</td>\n",
       "      <td>29</td>\n",
       "      <td>EFProdTotGHA</td>\n",
       "      <td>2.208863e+06</td>\n",
       "      <td>6.561413e+05</td>\n",
       "      <td>4231676.695</td>\n",
       "      <td>1.381187e+04</td>\n",
       "      <td>3.518474e+05</td>\n",
       "      <td>76016.397970</td>\n",
       "      <td>7.538357e+06</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  year  country_code        record     crop_land  grazing_land  \\\n",
       "20780   France  1992            68  EFConsPerCap  9.485874e-01  4.375167e-01   \n",
       "56478  Somalia  1965           201  EFConsPerCap  1.464603e-01  1.792926e+00   \n",
       "56887  Somalia  2016           201  EFConsTotGHA  1.723746e+06  5.411010e+06   \n",
       "12195    Chile  1995            40  BiocapTotGHA  5.149702e+06  7.393071e+06   \n",
       "9015   Burundi  2010            29  EFProdTotGHA  2.208863e+06  6.561413e+05   \n",
       "\n",
       "       forest_land  fishing_ground  built_up_land         carbon  \\\n",
       "20780  0.737739791    2.017436e-01   1.559900e-01       3.066358   \n",
       "56478  0.391813062    7.486020e-04   1.989028e-02       0.035686   \n",
       "56887  5865820.964    6.550573e+04   3.311375e+05  477086.269000   \n",
       "12195  35781483.45    6.758648e+06   1.007553e+06       0.000000   \n",
       "9015   4231676.695    1.381187e+04   3.518474e+05   76016.397970   \n",
       "\n",
       "              total QScore  \n",
       "20780  5.547935e+00     3A  \n",
       "56478  2.387525e+00     3A  \n",
       "56887  1.387431e+07     3A  \n",
       "12195  5.609046e+07     3A  \n",
       "9015   7.538357e+06     3A  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#order the dataset based on class 3A\n",
    "df_3A = df[df['QScore']=='3A'].sample(350)\n",
    "df_3A.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>year</th>\n",
       "      <th>country_code</th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1536</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>2.072989e-01</td>\n",
       "      <td>8.112722e-01</td>\n",
       "      <td>0.048357265</td>\n",
       "      <td>0.022585</td>\n",
       "      <td>2.998367e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.119497e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>AreaTotHA</td>\n",
       "      <td>8.417600e+06</td>\n",
       "      <td>3.294260e+07</td>\n",
       "      <td>1963600</td>\n",
       "      <td>917100.000000</td>\n",
       "      <td>1.217520e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.545842e+07</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1538</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>BiocapPerCap</td>\n",
       "      <td>2.021916e-01</td>\n",
       "      <td>2.636077e-01</td>\n",
       "      <td>0.027166736</td>\n",
       "      <td>0.007948</td>\n",
       "      <td>2.924496e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.301590e-01</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1539</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>8.210214e+06</td>\n",
       "      <td>1.070408e+07</td>\n",
       "      <td>1103135.245</td>\n",
       "      <td>322736.916200</td>\n",
       "      <td>1.187524e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.152769e+07</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>2016</td>\n",
       "      <td>4</td>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>6.280528e-01</td>\n",
       "      <td>1.810332e-01</td>\n",
       "      <td>0.162800822</td>\n",
       "      <td>0.014729</td>\n",
       "      <td>2.924496e-02</td>\n",
       "      <td>1.391455</td>\n",
       "      <td>2.407316e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      country  year  country_code        record     crop_land  grazing_land  \\\n",
       "1536  Algeria  2016             4    AreaPerCap  2.072989e-01  8.112722e-01   \n",
       "1537  Algeria  2016             4     AreaTotHA  8.417600e+06  3.294260e+07   \n",
       "1538  Algeria  2016             4  BiocapPerCap  2.021916e-01  2.636077e-01   \n",
       "1539  Algeria  2016             4  BiocapTotGHA  8.210214e+06  1.070408e+07   \n",
       "1540  Algeria  2016             4  EFConsPerCap  6.280528e-01  1.810332e-01   \n",
       "\n",
       "      forest_land  fishing_ground  built_up_land    carbon         total  \\\n",
       "1536  0.048357265        0.022585   2.998367e-02  0.000000  1.119497e+00   \n",
       "1537      1963600   917100.000000   1.217520e+06  0.000000  4.545842e+07   \n",
       "1538  0.027166736        0.007948   2.924496e-02  0.000000  5.301590e-01   \n",
       "1539  1103135.245   322736.916200   1.187524e+06  0.000000  2.152769e+07   \n",
       "1540  0.162800822        0.014729   2.924496e-02  1.391455  2.407316e+00   \n",
       "\n",
       "     QScore  \n",
       "1536     2A  \n",
       "1537     2A  \n",
       "1538     2A  \n",
       "1539     2A  \n",
       "1540     2A  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add df3 data into df2 data\n",
    "data_df = df_2A.append(df_3A)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(590, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.utils\n",
    "data_df = sklearn.utils.shuffle(data_df)\n",
    "data_df = data_df.reset_index(drop=True)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    350\n",
       "2A    240\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['QScore'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing\n",
    "data_df = data_df.drop(columns=['country_code','country','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record</th>\n",
       "      <th>crop_land</th>\n",
       "      <th>grazing_land</th>\n",
       "      <th>forest_land</th>\n",
       "      <th>fishing_ground</th>\n",
       "      <th>built_up_land</th>\n",
       "      <th>carbon</th>\n",
       "      <th>total</th>\n",
       "      <th>QScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BiocapPerCap</td>\n",
       "      <td>3.457978e-01</td>\n",
       "      <td>0.068372</td>\n",
       "      <td>0.059316204</td>\n",
       "      <td>1.872688e-01</td>\n",
       "      <td>6.998182e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.307371e-01</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AreaPerCap</td>\n",
       "      <td>1.517181e-01</td>\n",
       "      <td>0.433480</td>\n",
       "      <td>0.899254882</td>\n",
       "      <td>7.155026e-01</td>\n",
       "      <td>3.570794e-02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.235664e+00</td>\n",
       "      <td>2A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AreaTotHA</td>\n",
       "      <td>1.060000e+05</td>\n",
       "      <td>413000.000000</td>\n",
       "      <td>2764850</td>\n",
       "      <td>2.500000e+04</td>\n",
       "      <td>1.043440e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.413194e+06</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BiocapTotGHA</td>\n",
       "      <td>9.561259e+06</td>\n",
       "      <td>406877.207500</td>\n",
       "      <td>1.53246e+07</td>\n",
       "      <td>1.479154e+07</td>\n",
       "      <td>1.256865e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.134115e+07</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EFConsPerCap</td>\n",
       "      <td>7.024745e-01</td>\n",
       "      <td>0.802137</td>\n",
       "      <td>0.509670738</td>\n",
       "      <td>4.708482e-02</td>\n",
       "      <td>9.261041e-02</td>\n",
       "      <td>0.947393</td>\n",
       "      <td>3.101370e+00</td>\n",
       "      <td>3A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         record     crop_land   grazing_land  forest_land  fishing_ground  \\\n",
       "0  BiocapPerCap  3.457978e-01       0.068372  0.059316204    1.872688e-01   \n",
       "1    AreaPerCap  1.517181e-01       0.433480  0.899254882    7.155026e-01   \n",
       "2     AreaTotHA  1.060000e+05  413000.000000      2764850    2.500000e+04   \n",
       "3  BiocapTotGHA  9.561259e+06  406877.207500  1.53246e+07    1.479154e+07   \n",
       "4  EFConsPerCap  7.024745e-01       0.802137  0.509670738    4.708482e-02   \n",
       "\n",
       "   built_up_land    carbon         total QScore  \n",
       "0   6.998182e-02  0.000000  7.307371e-01     2A  \n",
       "1   3.570794e-02  0.000000  2.235664e+00     2A  \n",
       "2   1.043440e+05  0.000000  3.413194e+06     3A  \n",
       "3   1.256865e+06  0.000000  4.134115e+07     3A  \n",
       "4   9.261041e-02  0.947393  3.101370e+00     3A  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data_df.drop(columns='QScore')\n",
    "y = data_df['QScore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3A    244\n",
       "2A    169\n",
       "Name: QScore, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()\n",
    "#There is still an imbalance in the class distribution.\n",
    "#For this we use SMOTE technique only on the training data to handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encode categorical variable\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "x_train.record = encoder.fit_transform(x_train.record)\n",
    "y_train.record = encoder.transform(x_test.record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=1)\n",
    "x_train_balanced, y_balanced = smote.fit_resample(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalised_train_df = scaler.fit_transform(x_train_balanced.drop(columns=[ 'record' ]))\n",
    "normalised_train_df = pd.DataFrame(normalised_train_df,\n",
    "columns=x_train_balanced.drop(columns=[ 'record' ]).columns)\n",
    "normalised_train_df[ 'record' ] = x_train_balanced[ 'record' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reset_index(drop=True)\n",
    "normalised_test_df = scaler.transform(x_test.drop(columns=['record']))\n",
    "normalised_test_df = pd.DataFrame(normalised_test_df,columns=x_test.drop(columns=['record']).columns)\n",
    "normalised_test_df['record'] = x_test['record']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;1bfgs&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;1bfgs&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='1bfgs')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(normalised_train_df,y_balanced)\n",
    "LogisticRegression(C=1.0, class_weight=None,dual=False, fit_intercept=True,intercept_scaling=1,l1_ratio=None, max_iter=100,multi_class='auto',n_jobs=None,random_state=None,solver = '1bfgs',tol=0.0001,verbose=0,warm_start=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring classification performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation and accuracy\n",
    "\n",
    "Cross Validation is a well known and trusted method applied to avoid overfitting and enable generalization.\n",
    "\n",
    "Although there are different techniques used in performing cross validation, the fundamental concept involves partitioning the dataset into a number of subsets, holding out a set for evaluation then training the model on the other sets.\n",
    "\n",
    "This gives a more reliable estimate of how the model performs across different training sets because it provides an average score across different training samples used. \n",
    "\n",
    "The only drawback with cross-validation is that it takes more time and computation resources. However, the gain obtained in having a better model is very well worth this cost. \n",
    "\n",
    "Types of cross validation:\n",
    "1. K-Fold cross validation\n",
    "2. Stratified K-Fold cross validation \n",
    "3. Leave One Out Cross validation (LOOCV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.48421053, 0.4897605 , 0.51995831, 0.43806237, 0.50383632])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(log_reg,normalised_train_df,y_balanced,cv=5,scoring='f1_macro')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. K-Fold Cross Validation\n",
    "In this method, the data is split into 4 equal groups.\n",
    "If k=5, a 5 fold cross validation can be performed such that the data is split into k1,k2,k3,k4,k5\n",
    "The model is trained on k2-k5 and evaluated on k1 then repeated k times until every group is used to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold \n",
    "from sklearn.metrics import f1_score\n",
    "kf = KFold(n_splits=5)\n",
    "kf.split(normalised_train_df)\n",
    "f1_scores = []\n",
    "#run for every split\n",
    "for train_index,test_index in kf.split(normalised_train_df):\n",
    "    x_train,x_test=normalised_train_df.iloc[train_index],normalised_train_df.iloc[test_index]\n",
    "    y_train , y_test = y_balanced[train_index], y_balanced[test_index]\n",
    "    model = LogisticRegression().fit(x_train,y_train)\n",
    "    #save result to list\n",
    "    f1_scores.append(f1_score(y_true=y_test,y_pred=model.predict(x_test),pos_label='2A')*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stratefied K-Fold Cross Validation\n",
    "Although similar to the technique described above, Stratefied K-Fold cross validation ensures that in every fold, there is an equal proportion of each target class to obtain a good representation of the data and avoid imbalance and biased results. \n",
    "For example, if there are 2 target classes t1 and t2 with equal distribution in the data, it is best to ensure the folds also have the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "f1_scores=[]\n",
    "#run for every split\n",
    "for train_index, test_index in skf.split(normalised_train_df,y_balanced):\n",
    "    x_train,x_test = np.array(normalised_train_df)[train_index],np.array(normalised_train_df)[test_index]\n",
    "    y_train,y_test = y_balanced[train_index],y_balanced[test_index]\n",
    "    model=LogisticRegression().fit(x_train,y_train)\n",
    "#save result to list\n",
    "    f1_scores.append(f1_score(y_true=y_test,y_pred=model.predict(x_test),pos_label='2A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Leave One Out Cross Validation (LOOCV)\n",
    "One instance is left out and used as the test set while the model is trained on n-1 data points where N is the number of data points.\n",
    "This means that the number of instances and folds are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "loo = LeaveOneOut()\n",
    "scores = cross_val_score(LogisticRegression(),normalised_train_df,y_balanced,cv=loo,scoring='f1_macro')\n",
    "average_score = scores.mean()*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix, Precision-Recall, ROC curve, and F1-Score\n",
    "\n",
    "Accuracy, Precision, Recall, and F1-Score and many others are evaluation metrics used in measuring the performance of classification models. In this section, we discuss these metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix\n",
    "It is an n by n matrix that gives a summary of the correct and incorrect predicted classification results for the N target classes. \n",
    "\n",
    "The values in the diagonal of the matrix represent the number of correctly predicted classes while every other cell in the matrix indicates the misclassified classes. \n",
    "\n",
    "This means that the more predicted values that fall in the diagonal, the better the model. \n",
    "\n",
    "Terms in the confusion matrix:\n",
    "1. True positve\n",
    "This is a correct classification where the predicted value is the same as the actual value. Using the table above, this means that actual value was positive and the predicted value was also positive.\n",
    "2. False positive (type 1 error)\n",
    "This is a misclassification such that the model predicted a positive class while the actual class is negative. Telling a man that he is pregnant is definitely a false positive.\n",
    "3. True negative\n",
    "The predicted value also matches the actual value. In this case, it is for the negative class. The actual value is negative and the predicted value is negative.\n",
    "4. False negative (type 2 error)\n",
    "Also another misclassification where the predicted value is negative and the actual value is positive. Another example will be telling a pregnant women that she is not pregnant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score,accuracy_score,precision_score,f1_score,confusion_matrix \n",
    "new_predictions = log_reg.predict(normalised_test_df)\n",
    "cnf_mat = confusion_matrix(y_true=y_test)\n",
    "y_pred = new_predictions, label=['2A','3A']\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "It is the ratio of the number of correctly predicted instances to the total number of instances. \n",
    "It is a commenly used metric suitable when the target classes are not imbalanced. \n",
    "A high accuracy = does not necessarily mean that the model has high predicting power. Hence, depending on the task, it is important to not use only the accuracy metric because it does not provide enough information about the model.\n",
    "\n",
    "accuracy = (TP+TN)/(TP+TPN+FP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_true=y_test,y_pred=new_predictions)\n",
    "print('Accuracy: {}'.format(round(accuracy*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision\n",
    "\n",
    "The ratio of correctly predicted instances of a class to the total number of items predicted by the model to be in that class is referred to as precision (known as positive predicted value = ppv)\n",
    "This translates to the total percentage of the results obtained that are relevant. For the positive class, it is the ratio of true positives to the sum of true positives and false positives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_true=y_test,y_pred=new_predictions,pos_label='2A')\n",
    "print('Precision: {}'.format(round(precision*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall\n",
    "It is known as the sensitivity of the model, recall gives a % of total relevant results correctly predicted by the model. It is the ration of true positives to the actual number of positives (true positives and false negatives)\n",
    "\n",
    "recall = (TP)/(TP+FN)\n",
    "\n",
    "*There is also a trade off between precision and recall\n",
    "It is impossible to maximise both metrics at the same time because an increase in recall decreases precision.\n",
    "You must identify which metric is important based on your task and optimise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_true=y_test,y_pred=new_predictions,pos_label='2A')\n",
    "print('Recall: {}'.format(round(recall*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1-Score\n",
    "This is the harmonic mean of precision and recall that aims to have an optimal balance of both. \n",
    "The F1-Score is quite easy to use and can be focused on to maximise as opposed to maximizing precision and recall.\n",
    "\n",
    "F1 = 2*((Precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_true=y_test,y_pred=new_predictions,pos_label='2A')\n",
    "print('F1: {}'.format(ronud(f1*100),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC curve\n",
    "\n",
    "The Receiver Operating Characteristics (ROC) curve is a probability curve that measures the performance of a classification model at different set thresholds. \n",
    "Recall also known as the True Positive Rate (TPR) is plotted on the y-axis against the False Positive Rate (FPR) on the x-axis.\n",
    "\n",
    "The code examples above are not the optimal results that can be obtained with the model. Hyperparameter tuning can be performed to improve the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass classification\n",
    "\n",
    "Multilabel and Multiclass classification\n",
    "Multiclass classification deals with more than 2 classes where an instance is classified into a single class.\n",
    "i.e. given a dataset with a set of features that describe the weather such that the classes are sunny, rainy, and windy and give a single output like sunny \n",
    "\n",
    "In contrast, multilabel classification classifies an instance into a set of target labels.\n",
    "i.e. Articles and movies are examples where this can apply. An article can discuss a single topic but can also be about politics, religion, education, and many more while movies are commonly tagged to multiple genres such as comedy, adventure, and action.\n",
    "\n",
    "The softmax function is quite similar to the sigmoid function. It is used for multiclass classification because it can obtain the probabilities for various classes such that the probabilities of each class sum to 1. This means that an increase in the probability of a class causes a decrease in the probability of at least one of the other classes. It can also be referred to as a generalization of logistic regression or the sigmoid function can be used for multi class classification while the softmax function is used in multilabel classification.\n",
    "\n",
    "The softmax function is popularly used in the output layers of neural networks. Although, the sum of the outputs of the softmax must be 1, this is not the same for the sigmoid function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree-based methods and the support vector machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear and non-linear support vector machine\n",
    "\n",
    "Support Vector Machine is a supervised machine learning algorithm that is used to solve both classification and regression tasks\n",
    "\n",
    "In classification, the algorithm uses a line or hyperplane to separate classes by using data points close to the boundary (support vector) for each class and a hyperplane that maximizes the distance between the classes. \n",
    "\n",
    "Hyperplane - is a line that linearly separates data points. Although there can be several hyperplanes between classes, the optimal hyperplane which has the maximum distance or margin between itself and the support vectors is chosen. \n",
    "\n",
    "Data is not always linearly separable such that a straight line might not be able to adequately segregate classes.\n",
    "\n",
    "Although SVM is a linear classifier, it can be used to classify a non-linear dataset by transforming the dataset to a higher dimensional feature space where it can be linearly seperable.\n",
    "\n",
    "This is done using the kernel trick such that a kernel function is applied on each data point to map to a higher dimensional space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees and CART algorithm\n",
    "\n",
    "The decision tree is a widely used non-parametric supervised machine learning approach that splits instances in a dataset based on different decision rules inferred from the features in the dataset. \n",
    "It is a tree-based algorithm with nodes that represent a specific attribute or decision rule such that for an instance, a question is asked at a node and possible answers to the question found on both edges. \n",
    "\n",
    "This sequential process that involves recursive partitioning of nodes for several features until the leaves for the tree provides the final output or class for that instance. Decision tress can be used to solve regression and classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some examples of decision tree algorithms:\n",
    "1. Iterative Dichotomiser 3\n",
    "2. CART - Classification and Regression Trees\n",
    "3. C4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART algorithm\n",
    "\n",
    "The CART predictive model generates decision rules that have a binary tree representation such that each non-terminal node has 2 child nodes as opposed to some other tree-based methods that have more child nodes. \n",
    "It supports numerical target variables. \n",
    "At every node, the best split is chosen such that the splitting criterion is maximised. \n",
    "\n",
    "Gini impurity index is used as the splitting criterion in CART.\n",
    "\n",
    "Gini impurity - it is a measure of the chance that a randomly selected instance will be wrongly classified when selected. For different classes in a dataset, with p(i) as the probability that the chosen instance belongs to class i, the gini impurity index for all classes G, can be calculated such that:\n",
    "The values range between 0 and 1 where 0 means a pure classifcation where all instances belong to the same class and 1 means there is a random distribution of instances across different classes. \n",
    "To select the best split, the gini gain is calculated by taking a weighted sum of the gini impurity index then subtracting from the orginal impurity. \n",
    "Higher gini gain leads to better splits simply put the lower the gini impurity the better the split. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting in Decision Trees, Early stopping and pruning\n",
    "\n",
    "The recursive partitioninig of nodes until the final subsets are obtained in decision trees makes it prone to overfitting. \n",
    "The deeper the tree, the higher the chances of the overfitting. \n",
    "This can be prevented using a stopping criterion such as early stopping and pruning. \n",
    "Early stopping or pre-pruning involves stopping the tree-building process before the tree becomes too complex and the training data is perfectly classified. \n",
    "\n",
    "1. An early stopping condition like the maximum depth can be set to avoid deep trees such that the tree stops growing after reaching the set maximum depth for the tree. \n",
    "2. Another early stopping criterion that can be used is the classificatin error. At every splitting stage, the error is checked. If there is no significant decrease in the error, there is no need to make the tree more complex. \n",
    "When there are fewer data points than a set threshold value, early stopping can also take place. \n",
    "Early stopping may also produce underfit models if it stops too early. \n",
    "3. Post-pruning on the other hand, allows the tree to be fully built before simplifying by removing sections of the tree at different levels by calculating the error rate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dec_tree = DecisionTreeClassifier()\n",
    "dec_tree.fit(normalised_train_df,y_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods\n",
    "\n",
    "Beyond decision trees and ensemble classifiers:\n",
    "Ensembling in ML involves the combination of several classifiers to obtain an optimal model with better performance as opposed to just a single classifier. These classifiers can be of different algorithms and hyperparameters. \n",
    "\n",
    "Method classifiers can be combined:\n",
    "1. Bagging\n",
    "2. Boosting\n",
    "3. Stacking\n",
    "4. Blending"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Bagging \n",
    "\n",
    "Bootstrap Aggregation or Bagging is a parallel ensembling technique that randomly bootstraps or samples the dataset with replacement to create subsets from the original. Multiple models are then trained using these subsets and the predicted results from these models aggregated to return final predictions. Bagging results in a final model that has less variance than its base classifiers. \n",
    "\n",
    "- Bagging: Random Forests\n",
    "\n",
    "When bagging is applied to decision trees, it results in random forests which is a supervised learning algorithm that has a large number of decision trees. \n",
    "\n",
    "i.e in the dataset, each tree returns a prediction for the class that instance belongs to then, the class with the most votes becomes the final class for that instance. \n",
    "In random forests, it is assumed that a group of uncorrelated trees will do better than an individual tree. While some of the trees might be wrong in their predictions, many others will be correct. \n",
    "\n",
    "- Boosting: AdaBoost, Gradient Boosting and XGBoost\n",
    "\n",
    "1. Boosting\n",
    "Boosting is a sequential process where every phase attempts to correct the errors made by the previous model. The main principle is to fit multiple weak learners which are slightly better than just random guessing. In contrast to bagging, boosting attempts to reduce both variance and bias. \n",
    "\n",
    "Examples of boosting algorithms:\n",
    "1. AdaBoost\n",
    "2. Gradient Boosting\n",
    "3. XGBoost \n",
    "\n",
    "1. AdaBoost\n",
    "Adaptive Boosting is the first boosting algorithm. It is a very popular method for boosting that can be used on any classifier to present a more accurate model and improve its performance. It can be described with the following steps: create a subset from the entire dataset, assign equal weights to the data points, create a base model using this subset, predict using this model, calculate errors from the predicted results, assign higher weights to misclassified instances to increase their chances of being selected, create another model that tries to correct these mistakes and make new predictions then repeat until the maximum number of models specified are created. The final model is the weighted average of all the weak learners created. Adaboost is very sensitive to noisy data and outliers so it is important to remove these when using Adaboost.\n",
    "\n",
    "2. Gradient boosting\n",
    "This is another boosting algorithm that improves model performance where each model in the ensemble minimizes a loss function using gradient descent. The loss function which is used to obtain an estimate of how the model is performing, a weakl learner - a model only slightly better than random guessing typically decision stumps (a decision tree with a single split - one level) and an additive model that combines the weak learners to make the final model are 3 important components in gradient boosting. \n",
    "\n",
    "3. XGBoost\n",
    "Extreme gradient boosting is a supervised learning algorithm that implements gradient boosting by building trees parallely while applying regularization. It is well known for its scalability and fast execution. XGBoost can automatically identify missing values in data and it builds very deep trees before pruning for optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LIVE CODING SESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting heart disease using ML\n",
    "\n",
    "The notebook entails the classification of heart disease (if a patient has heart disease or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem definition\n",
    "\n",
    "The problem involves binary classification (a sample can only be one of 2 things), which involves using a number of different features (pieces of information) about a person to predict whether they have heart disease or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "\n",
    "The original data came from the cleveland database from UCI ML repository.\n",
    "The original database contains 76 attributes but here only 14 attributes were used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features\n",
    "\n",
    "There are 13 attributes\n",
    "\n",
    "1. age: age in years\n",
    "2. sex: sex (1 = male; 0 = female)\n",
    "3. cp: chest pain type\n",
    "-- Value 0: typical angina\n",
    "-- Value 1: atypical angina\n",
    "-- Value 2: non-anginal pain\n",
    "-- Value 3: asymptomatic\n",
    "4. trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n",
    "5. chol: serum cholestoral in mg/dl\n",
    "6. fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)\n",
    "7. restecg: resting electrocardiographic results\n",
    "-- Value 0: normal\n",
    "-- Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "-- Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "8. thalach: maximum heart rate achieved\n",
    "9. exang: exercise induced angina (1 = yes; 0 = no)\n",
    "10. oldpeak = ST depression induced by exercise relative to rest\n",
    "11. slope: the slope of the peak exercise ST segment\n",
    "-- Value 0: upsloping\n",
    "-- Value 1: flat\n",
    "-- Value 2: downsloping\n",
    "12. ca: number of major vessels (0-3) colored by flourosopy\n",
    "13. thal: 0 = normal; 1 = fixed defect; 2 = reversable defect\n",
    "and the label\n",
    "14. condition: 0 = no disease, 1 = disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tools\n",
    "\n",
    "#regular EDA and plotting libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "#Models from scikit learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Model Evaluations\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('heart_cleveland_upload.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['condition'].value_counts().plot(kind='bar',color=['blue','red'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sex.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.condition, df.sex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crosstab plot\n",
    "\n",
    "pd.crosstab(df.condition, df.sex).plot(kind='bar',color=['red','blue'],figsize=(10,6))\n",
    "plt.title('HEART DISEASE FREQUENCY FOR SEX')\n",
    "plt.xlabel('0 - no disease, 1 - disease')\n",
    "plt.ylabel('amount')\n",
    "plt.legend(['Female','Male'])\n",
    "plt.xticks(rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.thalach.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age vs. max heart rate for heart disease\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "#scatter with positive example\n",
    "plt.scatter(df.age[df.condition==1],df.thalach[df.condition==1],c='red')\n",
    "#scatter with negative example\n",
    "plt.scatter(df.age[df.condition==0],df.thalach[df.condition==0],c='blue')\n",
    "\n",
    "plt.title('Heart disease in function of age and max heart rate')\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('max heart rate')\n",
    "plt.legend('disease','no disease')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.age.plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heart disease frequency per chest pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.cp, df.condition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crosstab plot\n",
    "\n",
    "pd.crosstab(df.cp, df.condition).plot(kind='bar',color=['red','blue'],figsize=(8,6))\n",
    "plt.title('HEART DISEASE FREQUENCY PER CHEST PAIN TYPE')\n",
    "plt.xlabel('Chest pain type')\n",
    "plt.ylabel('Amount')\n",
    "plt.legend(['No disease','disease'])\n",
    "plt.xticks(rotation=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cp - chest pain type\n",
    "\n",
    "0: typical angina - chest pain related decreased blood supply to the heart\n",
    "1: atypical angina - chest pain not related to heart\n",
    "2: non-anginal pain - typically escophageal (non heart related)\n",
    "3: asymptomatic - chest pain not showing signs of disease    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the correlation more visual\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "ax = sns.heatmap(corr_mat,linewidth = 0.5, annot=True,cmap='BrBG_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data into x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=df.drop('condition', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df['condition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into train and test set\n",
    "\n",
    "np.random.seed(42)\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting models in a dictionary \n",
    "models = {\"Logistic Regression\":LogisticRegression(),\"KNN\":KNeighborsClassifier(),\"Random Forest\":RandomForestClassifier()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up a function to fit and score model\n",
    "def fit_and_score(models,x_train,x_test,y_train,y_test):\n",
    "    #setting up a random seed\n",
    "    np.random.seed(42)\n",
    "    #making a dictionary to keep model scores\n",
    "    model_scores={}\n",
    "    #looping through models\n",
    "    for name, model in models.items():\n",
    "        #fitting the model to the data\n",
    "        model.fit(x_train,y_train)\n",
    "        #evaluating the model and appending its score to model_scores\n",
    "        model_scores[name] = model.score(x_test,y_test)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = fit_and_score(models,x_train,x_test,y_train,y_test)\n",
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_compare = pd.DataFrame(model_scores, index=['Accuracy'])\n",
    "model_compare.T.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning\n",
    "Randomsearch is a method in which random combinations of hyperparameters are selected and used to train a model. \n",
    "The best random hyperparameter combinations are used. \n",
    "\n",
    "GridsearchCV is a technique to search through the best parameter values from the given set of the grid parameters. It is basically a cross-validation method. The model and the parameters are required to be fed in. Best parameter values are extracted and then the predictions are made. \n",
    "\n",
    "The only difference between both the approaches is in grid search we define the combinations and do training of the model whereas in RandomizedSearchCV the model selects the combinations randomly. Both are very effective ways of tuning the parameters that increase the model generalizability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperparameter tuning with random search CV\n",
    "\n",
    "Tuning:\n",
    "    1. Logistic Regression\n",
    "    2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a hyperparameter grid for logistic regression\n",
    "log_reg_grid = {'C':np.logspace(-4,4,20),'solver':['liblinear']}\n",
    "\n",
    "#creating a hyperparameter grid for random forest classifier\n",
    "rf_grid = {'n_estimators':np.arange(10,1000,50),\n",
    "           'max_depth':[None,3,5,10],\n",
    "           'min_samples_split':np.arange(2,20,2),\n",
    "           'min_samples_leaf':np.arange(1,20,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning logistic regression\n",
    "\n",
    "np.random.seed(42)\n",
    "#setup random hyperparameter search for logisticregression\n",
    "rs_log_reg = RandomizedSearchCV(LogisticRegression(),param_distributions=log_reg_grid,cv=5,n_iter=20,verbose=True)\n",
    "\n",
    "#fitting random hyperparameter search model for logisticregression\n",
    "rs_log_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_log_reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuning random forest classifier\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "#setup random hyperparameter search for randomforest classifier\n",
    "\n",
    "rs_rf = RandomizedSearchCV(RandomForestClassifier(), param_distributions=rf_grid,cv=5,n_iter=30,verbose=True)\n",
    "\n",
    "#fitting random hyperparameter search model for RandomForestClassifier\n",
    "rs_rf.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs_rf.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hyperparameter tuning with GridSearchCV\n",
    "\n",
    "Since logistic regression model performed better, we will try to improve it again using GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#different hyperparameters for the logistic regression model:\n",
    "\n",
    "log_reg_grid = {'C':np.logspace(-4,4,30),\n",
    "               'solver':['liblinear']}\n",
    "\n",
    "np.random.seed(42)\n",
    "#setup Grid hyperparameter search for logisticregression\n",
    "gs_log_reg = GridSearchCV(LogisticRegression(),param_grid=log_reg_grid,cv=5,verbose=True)\n",
    "\n",
    "#fitting grid hyperparameters search model for logisticregression\n",
    "gs_log_reg.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_log_reg.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_log_reg.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating the tuned machine learning classifier, beyond accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making predictions with tuned model\n",
    "y_preds = gs_log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion matrix\n",
    "\n",
    "#A confusion matrix is a tabular representation that summarizes the predictions of a classification model against the actual labels. \n",
    "#It provides a comprehensive view of the model's performance by breaking down the predictions into 4 categories:\n",
    "\n",
    "1. True Positives (TP): The model correctly predicted positive instances as\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = (tp) / (tp+fn)\n",
    "\n",
    "fpr = (fp) / (fp+tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "\n",
    "preds = model.predict(x_test)\n",
    "fpr,tpr, thresholds = roc_curve(y_test,preds, pos_label=1)\n",
    "auc=roc_auc_score(y_test,preds)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "ax.plot(fpr,tpr)\n",
    "ax.plot([0,1],[0,1],color='navy',linestyle='--',label='random')\n",
    "plt.title(f'AUC:{auc}')\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "weights = compute_class_weight('balanced', classes, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
